# kubeadm部署K8s集群

kubeadm：是社区发起的一个独立的部署k8s的工具



## 整体流程

1. 在`所有节点`上安装 Docker 和 kubeadm
2. 部署 Kubernetes Master
3. 部署容器网络插件
4. 部署 Kubernetes Worker
5. 部署 Dashboard 可视化插件
6. 部署容器存储插件



## 满足条件的机器

1. 满足安装 Docker 项目所需的要求（比如 64 位的 Linux 操作系统、3.10 及以上的内核版本）

2. x86 或者 ARM 架构

3. **机器之间网络互通**，这是将来容器之间网络互通的前提

4. 有外网访问权限，因为需要拉取镜像

5. 能够访问到gcr.io、quay.io这两个 docker registry（有个别镜像需要在这里拉取）

6. 单机可用资源建议 2 核 CPU、4GB内存以上

7. 30 GB 或以上的可用磁盘空间

   > 给Docker镜像和日志文件使用

   

## 腾讯云服务器 部署k8s

### 硬件要求

```
cpu 2核
内存 2G或4G

各腾讯云服务器内网互通（使注意：腾讯云不同账号下的主机的内网是不互通的，需要购买“云联网”或“对等连接”服务）
```

### 软件环境要求

```bash
root@VM-0-10-ubuntu:/home/ubuntu# lsb_release -a
No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 18.04.6 LTS
Release:	18.04
Codename:	bionic
```

### Master和Node节点安装环境

```bash
# 关闭防火墙
ufw disable

# 永久关闭 swap 分区
sed -i 's/.*swap.*/#&/' /etc/fstab

# 1、添加aliyun的key，不然运行会报The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB
$ curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add

# 2、写入阿里云的的ubuntu镜像源到kubernetes.list文件
$ cat <<EOF > /etc/apt/sources.list.d/kubernetes.list
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
EOF
# 或在/etc/apt/sources.list文件添加一行也行 deb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main

# 更新软件包
$ apt-get update

# 安装docker 和 kubeadm(kubeadm、kubelet、kubectl、kubernetes-cni 这几个二进制文件都会被自动安装好)
$ apt-get install -y docker.io kubeadm

# 此处k8s安装的是v1.23.4版本 docker是20.10.7版本
```

#### 创建master节点

##### Kubeadm init

```bash
# 在master执行以下命令
# 注意--apiserver-advertise-address此处为私网ip 172.17.0.10
# --image-repository指定使用阿里云的镜像，不然国内下载k8s.gcr.io镜像太慢
kubeadm init --image-repository=registry.aliyuncs.com/google_containers --apiserver-advertise-address=172.17.0.10 --kubernetes-version=1.23.4 --service-cidr=10.96.0.0/12 --pod-network-cidr=10.244.0.0/16

# kubeadm init 可以带以下参数
--kubernetes-version  # 指定要安装的k8s版本(执行kubelet --version查看)
--pod-network-cidr    # 指定pod网络地址范围，固定 10.244.0.0/16 就行
--service-cidr   # 指定service网络地址范围，固定 10.96.0.0/12 就行
--apiserver-advertise-address #指定api地址，这里默认配置成了腾讯云服务器master的私网IP
```

如果init过程遇到什么错，可以执行`kubeadm reset`清空信息重新来



**可能遇到的问题1:**

![](https://gitee.com/sinkhaha/picture/raw/master/img/CICD/20220223220027.png)

查看错误日志命令：

```bash
journalctl -xeu kubelet

或

# 查看状态信息，也有日志信息
systemctl status kubelet -l
```

错误日志如下：

![](https://gitee.com/sinkhaha/picture/raw/master/img/CICD/20220226164838.png)

原因：kubelet cgroup driver(默认为system)和docker cgroup driver(默认为cgroupfs)的驱动不一致

解决方式：统一kubelet和docker的cgroup driver即可，修改docker的cgroup driver为systemd（现在Kubernetes推荐使用 systemd 来代替 cgroupfs）

```bash
# 查看docker的cgroup驱动
docker info|grep "Cgroup Driver"

# 执行修改docker的cgroup驱动为systemd
echo '{"exec-opts": ["native.cgroupdriver=systemd"]}' | sudo tee /etc/docker/daemon.json

# 为使配置生效，重启docker
systemctl daemon-reload
systemctl restart docker
```

也可以修改k8s的cgroup driver为cgroupfs，参考https://kubernetes.io/zh/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/



**接着重新init主节点**

```bash
# reset
kubeadm reset 

# 重新init
kubeadm init --image-repository=registry.aliyuncs.com/google_containers --apiserver-advertise-address=172.17.0.10 --kubernetes-version=1.23.4 --service-cidr=10.96.0.0/12 --pod-network-cidr=10.244.0.0/16
```



**Init成功后输出**

```bash
# 初始化master成功后 输出，找个地方记一下，等会从节点加入可以用 
......
kubeadm join 172.17.0.10:6443 --token l3nybz.0egeyw5uhvtdz201 \
	--discovery-token-ca-cert-hash sha256:0fee4f0ecfba20f067a6037f6eea24f5b30c1a1e0b924e0e5009a45e38f5a9bd


# 注意：这个token有效期只有24h 过期要重新在master节点新建
kubeadm token create --print-join-command
```



**master初始化成功后，继续执行**

```bash
# kubeadm 还会提示我们第一次使用 Kubernetes 集群所需要的配置命令
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```



**查看master的状态**

```bash
# 查看当前唯一master节点的状态
kubectl get nodes

# 输出如下
NAME             STATUS     ROLES                  AGE     VERSION
vm-0-10-ubuntu   NotReady   control-plane,master   5m56s   v1.23.4

# NotReady状态是因为未安装网络插件


# 查看各个pod的状态
kubectl get pods -n kube-system

NAME                                     READY   STATUS    RESTARTS   AGE
coredns-6d8c4cb4d-2x4tb                  0/1     Pending   0          5m43s
coredns-6d8c4cb4d-tqw5q                  0/1     Pending   0          5m43s
etcd-vm-0-10-ubuntu                      1/1     Running   0          5m59s
kube-apiserver-vm-0-10-ubuntu            1/1     Running   0          5m58s
kube-controller-manager-vm-0-10-ubuntu   1/1     Running   10         5m58s
kube-proxy-p7qrf                         1/1     Running   0          5m44s
kube-scheduler-vm-0-10-ubuntu            1/1     Running   10         5m56s
```



> 因为Kubernetes 的 Taint/Toleration 机制，默认情况下 Master 节点是`不允许`运行用户Pod的。
>
> 它的原理非常简单：一旦某个节点被加上了一个 Taint，即被“打上了污点”，那么所有 Pod 就都不能在这个节点上运行，因为 Kubernetes 的 Pod 都有“洁癖”。



##### 主节点部署网络插件Weave

```bash
# 推荐
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"

或

kubectl apply -f https://git.io/weave-kube-1.6


# 获取pods的状态，都是running
kubectl get pods -n kube-system
NAME                                     READY   STATUS    RESTARTS      AGE
coredns-6d8c4cb4d-2x4tb                  1/1     Running   0             25m
coredns-6d8c4cb4d-tqw5q                  1/1     Running   0             25m
etcd-vm-0-10-ubuntu                      1/1     Running   0             25m
kube-apiserver-vm-0-10-ubuntu            1/1     Running   0             25m
kube-controller-manager-vm-0-10-ubuntu   1/1     Running   10            25m
kube-proxy-p7qrf                         1/1     Running   0             25m
kube-scheduler-vm-0-10-ubuntu            1/1     Running   10            25m
weave-net-b6dt5                          2/2     Running   1 (10m ago)   17m
```



#### 创建Node节点

Kubernetes 的 Worker 节点跟 Master 节点几乎是相同的，它们运行着的都是一个 kubelet 组件。

> 唯一的区别在于，在 kubeadm init 的过程中，kubelet 启动后，Master 节点上还会自动运行 kube-apiserver、kube-scheduler、kube-controller-manger 这三个系统 Pod



**部署Worker节点只需要两步即可完成**

1. 在所有 Worker 节点`“安装 kubeadm 和 Docker”`

2. 执行部署 Master 节点时生成的 kubeadm join 指令：

```bash
# 将一个Node节点加入到当前集群中
$ kubeadm join <Master节点的ip:6443> --token <token的值> --discovery-token-ca-cert-hash sha256:<ca的哈希值>

# 这个token有效期只有24h 过期则重新在master节点新建
kubeadm token create --print-join-command
```

前提：Worker跟Master节点网络互通，可在从节点运行以下命令查看和master的端口是否互通

```bash
nc -zv <master> 端口
如nc -zv 172.17.0.10 6443
```



### 部署 Dashboard 可视化插件

Dashboard 项目，部署如下

```bash
$ kubectl apply -f 
$ $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-rc6/aio/deploy/recommended.yaml
```

部署完成之后，可以查看 Dashboard 对应的 Pod 的状态

```bash
$ kubectl get pods -n kube-system

kubernetes-dashboard-6948bdb78-f67xk   1/1       Running   0          1m
```



1.7 版本之后的 Dashboard 项目部署完成后，默认只能通过 Proxy 的方式在本地访问

如果想从集群外访问这个 Dashboard 的话，就需要用到 Ingress。

略 // TODO



### 部署容器存储插件Rook

> 容器持久化存储：用来`保存容器存储状态`的重要手段

提供持久化存储能力的项目： Ceph、GlusterFS、NFS、Rook 等



**持久化的含义**

存储插件会在容器里`挂载`一个基于网络或者其他机制的`远程数据卷`，使得在容器里创建的文件，实际上是`保存在远程存储服务器上`，或者以分布式的方式保存在多个节点上，而与当前宿主机没有任何绑定关系。

> 这样无论在其他哪个宿主机上启动新的容器，都可以请求挂载指定的持久化存储卷，从而访问到数据卷里保存的内容



**部署Kubernetes 存储插件项目Rook**

> Rook 项目是一个基于 Ceph 的 Kubernetes 存储插件。
>
> 不同于对 Ceph 的简单封装，Rook 在自己的实现中加入了水平扩展、迁移、灾难备份、监控等大量的企业级功能，使得这个项目变成了一个完整的、生产级别可用的容器存储插件。



```bash
$ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/common.yaml

$ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/operator.yaml

$ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/cluster.yaml
```

在部署完成后，可以看到 Rook 项目会将自己的 Pod 放置在由它自己管理的两个 Namespace 当中：

```bash
$ kubectl get pods -n rook-ceph-system
NAME                                  READY     STATUS    RESTARTS   AGE
rook-ceph-agent-7cv62                 1/1       Running   0          15s
rook-ceph-operator-78d498c68c-7fj72   1/1       Running   0          44s
rook-discover-2ctcv                   1/1       Running   0          15s

$ kubectl get pods -n rook-ceph
NAME                   READY     STATUS    RESTARTS   AGE
rook-ceph-mon0-kxnzh   1/1       Running   0          13s
rook-ceph-mon1-7dn2t   1/1       Running   0          2s
```

一个基于 Rook 持久化存储集群就以`容器的方式`运行起来了，接下来在 Kubernetes 项目上创建的所有 Pod 就能够通过 Persistent Volume（PV）和 Persistent Volume Claim（PVC）的方式，在容器里挂载由 Ceph 提供的数据卷了。

