# 一、kubeadm部署k8s集群

kubeadm：是社区发起的一个独立的部署k8s的工具



## 为什么不用容器部署k8s

> 即给每个k8s组件做一个容器镜像，然后在每台宿主机上用 docker run 指令启动这些组件容器

答：因为容器化kubelet会带来很多问题，不推荐用容器去部署 Kubernetes 项目



1. kubelet 是k8s用来操作 Docker 等容器运行时的核心组件

2. kubelet 在配置容器网络、管理容器数据卷时，都需要`直接操作`宿主机

3. 如果 kubelet 本身运行在一个容器里，那么直接操作宿主机就会变得很麻烦

   > 对于网络配置来说，kubelet 容器可以通过不开启 Network Namespace（即 Docker 的 host network 模式）的方式，直接共享宿主机的网络栈。
   >
   > 但是要让 kubelet 隔着容器的 Mount Namespace 和文件系统，操作宿主机的文件系统，就有点困难

4. kubelet 在k8s项目中的地位非常高，在设计上它就是一个`完全独立的组件`，而其他 Master 组件，则更像是辅助性的系统容器



## 部署k8s集群命令

Kubeadm通过kubeadm init和 kubeadm join能完成一个 k8s集群的部署，如下

```bash
# 创建一个Master节点
$ kubeadm init

# 将一个Node节点加入到当前集群中
$ kubeadm join <Master节点的IP和端口>
```



## kubeadm init的工作流程

1. 执行kubeadm init命令

2. Preflight Checks操作

   > kubeadm 做一系列的检查工作，检查机器是否可以部署 Kubernetes

3. 生成k8s对外提供服务所需的`各种证书`和`对应的目录`

   > Kubernetes 对外提供服务时，都要通过 HTTPS 才能访问 kube-apiserver，除非专门开启“不安全模式”。
   >
   > 
   >
   > 生成的证书文件都放在 Master 节点的 /etc/kubernetes/pki 目录下，最主要的证书文件是 ca.crt 和对应的私钥 ca.key。
   >
   > 
   >
   > 使用 kubectl 获取容器日志等 streaming 操作时，需要通过 kube-apiserver 向 kubelet 发起请求，这个连接也必须是安全的。kubeadm 为这一步生成的是 apiserver-kubelet-client.crt 文件，对应的私钥是 apiserver-kubelet-client.key

4. 证书生成后，为其他组件生成访问 kube-apiserver 所需的配置文件

   > 这些文件的路径是：/etc/kubernetes/xxx.conf
   >
   > 这些文件记录的是当前这个 Master 节点的服务器地址、监听端口、证书目录等信息

5. 为 Master 组件生成 Pod 配置文件

   > Kubernetes 有三个 Master 组件 kube-apiserver、kube-controller-manager、kube-scheduler，而它们都会被使用 Pod 的方式部署起来

6. 生成YAML文件，Master 容器启动

   > k8s有一种特殊的容器启动方法叫做“Static Pod”。
   >
   > 
   >
   > 它允许你把要部署的 Pod 的 YAML 文件放在一个指定的目录里。这样，当这台机器上的 kubelet 启动时，它会自动检查这个目录，加载所有的 Pod YAML 文件，然后在这台机器上启动它们。
   >
   > 

   > 在 kubeadm 中，Master 组件的 YAML 文件会被生成在 /etc/kubernetes/manifests 路径下。比如，kube-apiserver.yaml。
   >
   > 
   >
   > kubeadm 还会再生成一个 Etcd 的 Pod YAML 文件，用来通过同样的 Static Pod 的方式启动 Etcd。
   >
   > 
   >
   > 而一旦这些 YAML 文件出现在被 kubelet 监视的 /etc/kubernetes/manifests 目录下，kubelet 就会自动创建这些 YAML 文件中定义的 Pod，即 Master 组件的容器。

7. 为集群生成一个 bootstrap token

   > 只要持有这个 token，任何一个安装了 kubelet 和 kubadm 的节点，都可以通过 kubeadm join 加入到这个集群当中

8. 在 token 生成之后，kubeadm 会将 ca.crt 等 Master 节点的重要信息，通过 ConfigMap 的方式保存在 Etcd 当中，供后续部署 Node 节点使用

   > 这个 ConfigMap 的名字是 cluster-info

9. kubeadm init 的最后一步，就是安装默认插件

   > Kubernetes 默认 kube-proxy 和 DNS 这两个插件是必须安装的。它们分别用来提供整个集群的服务发现和 DNS 功能

   

## kubeadm join 的工作流程

kubeadm init 生成 bootstrap token 之后，可以在任意一台安装了 kubelet 和 kubeadm 的机器上执行 kubeadm join 了。



**为什么执行 kubeadm join 需要这样一个 token**

> 任何一台机器想要成为 Kubernetes 集群中的一个节点，就必须在集群的 kube-apiserver 上注册。要想跟 apiserver 打交道，这台机器就必须要获取到相应的证书文件（CA 文件）。

为了能一键安装，就不能让用户去 Master 节点上手动拷贝这些文件。

所以，kubeadm 至少需要发起一次“不安全模式”的访问到 kube-apiserver，从而拿到保存在 ConfigMap 中的 cluster-info（它保存了 APIServer 的授权信息）。而 bootstrap token，扮演的就是这个过程中的安全验证的角色。



有了 cluster-info 里的 kube-apiserver 的地址、端口、证书，kubelet 就可以以“安全模式”连接到 apiserver 上，这样一个新的节点就部署完成了。



# 二、实践部署K8s集群

## 整体流程

1. 在`所有节点`上安装 Docker 和 kubeadm
2. 部署 Kubernetes Master
3. 部署容器网络插件
4. 部署 Kubernetes Worker
5. 部署 Dashboard 可视化插件
6. 部署容器存储插件



## 满足条件的机器

1. 满足安装 Docker 项目所需的要求（比如 64 位的 Linux 操作系统、3.10 及以上的内核版本）

2. x86 或者 ARM 架构

3. **机器之间网络互通**，这是将来容器之间网络互通的前提

4. 有外网访问权限，因为需要拉取镜像

5. 能够访问到gcr.io、quay.io这两个 docker registry（有个别镜像需要在这里拉取）

6. 单机可用资源建议 2 核 CPU、4GB内存以上

7. 30 GB 或以上的可用磁盘空间

   > 给Docker镜像和日志文件使用

   

## 腾讯云服务器 部署k8s

### 硬件要求

```
cpu 2核
内存 2G或4G

各腾讯云服务器内网互通（使注意：腾讯云不同账号下的主机的内网是不互通的，需要购买“云联网”或“对等连接”服务）
```

### 软件环境要求

```bash
root@VM-0-10-ubuntu:/home/ubuntu# lsb_release -a
No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 18.04.6 LTS
Release:	18.04
Codename:	bionic
```

### Master和Node节点安装环境

```bash
# 关闭防火墙
ufw disable

# 永久关闭 swap 分区
sed -i 's/.*swap.*/#&/' /etc/fstab

# 1、添加aliyun的key，不然运行会报The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB
$ curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add

# 2、写入阿里云的的ubuntu镜像源到kubernetes.list文件
$ cat <<EOF > /etc/apt/sources.list.d/kubernetes.list
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
EOF
# 或在/etc/apt/sources.list文件添加一行也行 deb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main

# 更新软件包
$ apt-get update

# 安装docker 和 kubeadm(kubeadm、kubelet、kubectl、kubernetes-cni 这几个二进制文件都会被自动安装好)
$ apt-get install -y docker.io kubeadm

# 此处k8s安装的是v1.23.4版本 docker是20.10.7版本
```

#### 创建master节点

##### Kubeadm init

```bash
# 在master执行以下命令
# 注意--apiserver-advertise-address此处为私网ip 172.17.0.10
# --image-repository指定使用阿里云的镜像，不然国内下载k8s.gcr.io镜像太慢
kubeadm init --image-repository=registry.aliyuncs.com/google_containers --apiserver-advertise-address=172.17.0.10 --kubernetes-version=1.23.4 --service-cidr=10.96.0.0/12 --pod-network-cidr=10.244.0.0/16

# kubeadm init 可以带以下参数
--kubernetes-version  # 指定要安装的k8s版本(执行kubelet --version查看)
--pod-network-cidr    # 指定pod网络地址范围，固定 10.244.0.0/16 就行
--service-cidr   # 指定service网络地址范围，固定 10.96.0.0/12 就行
--apiserver-advertise-address #指定api地址，这里默认配置成了腾讯云服务器master的私网IP


# kubeadm init后面也可以指定一个YAML文件，在里面填写各种自定义的部署参数
# 如 kubeadm init --config kubeadm.yaml
```

如果init过程遇到什么错，可以执行`kubeadm reset`清空信息重新来



**可能遇到的问题1:**

![](https://gitee.com/sinkhaha/picture/raw/master/img/CICD/20220223220027.png)

查看错误日志命令：

```bash
journalctl -xeu kubelet

或

# 查看状态信息，也有日志信息
systemctl status kubelet -l
```

错误日志如下：

![](https://gitee.com/sinkhaha/picture/raw/master/img/CICD/20220226164838.png)

原因：kubelet cgroup driver(默认为system)和docker cgroup driver(默认为cgroupfs)的驱动不一致

解决方式：统一kubelet和docker的cgroup driver即可，修改docker的cgroup driver为systemd（现在Kubernetes推荐使用 systemd 来代替 cgroupfs）

```bash
# 查看docker的cgroup驱动
docker info|grep "Cgroup Driver"

# 执行修改docker的cgroup驱动为systemd
echo '{"exec-opts": ["native.cgroupdriver=systemd"]}' | sudo tee /etc/docker/daemon.json

# 为使配置生效，重启docker
systemctl daemon-reload
systemctl restart docker
```

也可以修改k8s的cgroup driver为cgroupfs，参考https://kubernetes.io/zh/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/



**接着重新init主节点**

```bash
# reset
kubeadm reset 

# 重新init
kubeadm init --image-repository=registry.aliyuncs.com/google_containers --apiserver-advertise-address=172.17.0.10 --kubernetes-version=1.23.4 --service-cidr=10.96.0.0/12 --pod-network-cidr=10.244.0.0/16
```



**Init成功后输出**

```bash
# 初始化master成功后 输出，找个地方记一下，等会从节点加入可以用 
......
kubeadm join 172.17.0.10:6443 --token l3nybz.0egeyw5uhvtdz201 \
	--discovery-token-ca-cert-hash sha256:0fee4f0ecfba20f067a6037f6eea24f5b30c1a1e0b924e0e5009a45e38f5a9bd


# 注意：这个token有效期只有24h 过期要重新在master节点新建
kubeadm token create --print-join-command
```



**master初始化成功后，继续执行**

```bash
# kubeadm 还会提示我们第一次使用 Kubernetes 集群所需要的配置命令
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```



**查看master的状态**

```bash
# 查看当前唯一master节点的状态
kubectl get nodes

# 输出如下
NAME             STATUS     ROLES                  AGE     VERSION
vm-0-10-ubuntu   NotReady   control-plane,master   5m56s   v1.23.4

# NotReady状态是因为未安装网络插件


# 查看各个pod的状态
kubectl get pods -n kube-system

NAME                                     READY   STATUS    RESTARTS   AGE
coredns-6d8c4cb4d-2x4tb                  0/1     Pending   0          5m43s
coredns-6d8c4cb4d-tqw5q                  0/1     Pending   0          5m43s
etcd-vm-0-10-ubuntu                      1/1     Running   0          5m59s
kube-apiserver-vm-0-10-ubuntu            1/1     Running   0          5m58s
kube-controller-manager-vm-0-10-ubuntu   1/1     Running   10         5m58s
kube-proxy-p7qrf                         1/1     Running   0          5m44s
kube-scheduler-vm-0-10-ubuntu            1/1     Running   10         5m56s
```



> 因为Kubernetes 的 Taint/Toleration 机制，默认情况下 Master 节点是`不允许`运行用户Pod的。
>
> 它的原理非常简单：一旦某个节点被加上了一个 Taint，即被“打上了污点”，那么所有 Pod 就都不能在这个节点上运行，因为 Kubernetes 的 Pod 都有“洁癖”。



##### 主节点部署网络插件Weave

```bash
# 推荐
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"

或

kubectl apply -f https://git.io/weave-kube-1.6


# 获取pods的状态，都是running
kubectl get pods -n kube-system
NAME                                     READY   STATUS    RESTARTS      AGE
coredns-6d8c4cb4d-2x4tb                  1/1     Running   0             25m
coredns-6d8c4cb4d-tqw5q                  1/1     Running   0             25m
etcd-vm-0-10-ubuntu                      1/1     Running   0             25m
kube-apiserver-vm-0-10-ubuntu            1/1     Running   0             25m
kube-controller-manager-vm-0-10-ubuntu   1/1     Running   10            25m
kube-proxy-p7qrf                         1/1     Running   0             25m
kube-scheduler-vm-0-10-ubuntu            1/1     Running   10            25m
weave-net-b6dt5                          2/2     Running   1 (10m ago)   17m
```



#### 创建Node节点

Kubernetes 的 Worker 节点跟 Master 节点几乎是相同的，它们运行着的都是一个 kubelet 组件。

> 唯一的区别在于，在 kubeadm init 的过程中，kubelet 启动后，Master 节点上还会自动运行 kube-apiserver、kube-scheduler、kube-controller-manger 这三个系统 Pod



**部署Worker节点只需要两步即可完成**

1. 在所有 Worker 节点`“安装 kubeadm 和 Docker”`

2. 执行部署 Master 节点时生成的 kubeadm join 指令：

```bash
# 将一个Node节点加入到当前集群中
$ kubeadm join <Master节点的ip:6443> --token <token的值> --discovery-token-ca-cert-hash sha256:<ca的哈希值>

# 这个token有效期只有24h 过期则重新在master节点新建
kubeadm token create --print-join-command
```

前提：Worker跟Master节点网络互通，可在从节点运行以下命令查看和master的端口是否互通

```bash
nc -zv <master> 端口
如nc -zv 172.17.0.10 6443
```



### 部署 Dashboard 可视化插件

Dashboard 项目，部署如下

```bash
$ kubectl apply -f 
$ $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-rc6/aio/deploy/recommended.yaml
```

部署完成之后，可以查看 Dashboard 对应的 Pod 的状态

```bash
$ kubectl get pods -n kube-system

kubernetes-dashboard-6948bdb78-f67xk   1/1       Running   0          1m
```

1.7 版本之后的 Dashboard 项目部署完成后，默认只能通过 Proxy 的方式在本地访问

如果想从集群外访问这个 Dashboard 的话，就需要用到 Ingress。

> 可见另一章Ingress学习相关知识



### 部署容器存储插件Rook

> 容器持久化存储：用来`保存容器存储状态`的重要手段

提供持久化存储能力的项目： Ceph、GlusterFS、NFS、Rook 等



**持久化的含义**

存储插件会在容器里`挂载`一个基于网络或者其他机制的`远程数据卷`，使得在容器里创建的文件，实际上是`保存在远程存储服务器上`，或者以分布式的方式保存在多个节点上，而与当前宿主机没有任何绑定关系。

> 这样无论在其他哪个宿主机上启动新的容器，都可以请求挂载指定的持久化存储卷，从而访问到数据卷里保存的内容



**部署k8s 存储插件项目Rook**

> Rook项目是一个基于Ceph的k8s存储插件。
>
> 不同于对 Ceph 的简单封装，Rook 在自己的实现中加入了水平扩展、迁移、灾难备份、监控等大量的企业级功能，使得这个项目变成了一个完整的、生产级别可用的容器存储插件。

```bash
$ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/common.yaml

$ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/operator.yaml

$ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/cluster.yaml
```

在部署完成后，可以看到 Rook 项目会将自己的 Pod 放置在由它自己管理的两个 Namespace 当中：

```bash
$ kubectl get pods -n rook-ceph-system
NAME                                  READY     STATUS    RESTARTS   AGE
rook-ceph-agent-7cv62                 1/1       Running   0          15s
rook-ceph-operator-78d498c68c-7fj72   1/1       Running   0          44s
rook-discover-2ctcv                   1/1       Running   0          15s

$ kubectl get pods -n rook-ceph
NAME                   READY     STATUS    RESTARTS   AGE
rook-ceph-mon0-kxnzh   1/1       Running   0          13s
rook-ceph-mon1-7dn2t   1/1       Running   0          2s
```

一个基于 Rook 持久化存储集群就以`容器的方式`运行起来了，接下来在k8s项目上创建的所有 Pod 就能够通过 Persistent Volume（PV）和 Persistent Volume Claim（PVC）的方式，在容器里挂载由 Ceph 提供的数据卷了。

